{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/cu111\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.10.0+cu111)\n",
      "Collecting torch\n",
      "  Downloading torch-1.11.0-cp37-cp37m-manylinux1_x86_64.whl (750.6 MB)\n",
      "\u001B[K     |████████████████████████████████| 750.6 MB 12 kB/s \n",
      "\u001B[?25hRequirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (0.11.1+cu111)\n",
      "Collecting torchvision\n",
      "  Downloading torchvision-0.12.0-cp37-cp37m-manylinux1_x86_64.whl (21.0 MB)\n",
      "\u001B[K     |████████████████████████████████| 21.0 MB 11.5 MB/s \n",
      "\u001B[?25hRequirement already satisfied: torchaudio in /usr/local/lib/python3.7/dist-packages (0.10.0+cu111)\n",
      "Collecting torchaudio\n",
      "  Downloading torchaudio-0.11.0-cp37-cp37m-manylinux1_x86_64.whl (2.9 MB)\n",
      "\u001B[K     |████████████████████████████████| 2.9 MB 35.3 MB/s \n",
      "\u001B[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (3.10.0.2)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchvision) (2.23.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision) (1.21.5)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision) (7.1.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision) (2021.10.8)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision) (1.24.3)\n",
      "Installing collected packages: torch, torchvision, torchaudio\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 1.10.0+cu111\n",
      "    Uninstalling torch-1.10.0+cu111:\n",
      "      Successfully uninstalled torch-1.10.0+cu111\n",
      "  Attempting uninstall: torchvision\n",
      "    Found existing installation: torchvision 0.11.1+cu111\n",
      "    Uninstalling torchvision-0.11.1+cu111:\n",
      "      Successfully uninstalled torchvision-0.11.1+cu111\n",
      "  Attempting uninstall: torchaudio\n",
      "    Found existing installation: torchaudio 0.10.0+cu111\n",
      "    Uninstalling torchaudio-0.10.0+cu111:\n",
      "      Successfully uninstalled torchaudio-0.10.0+cu111\n",
      "\u001B[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torchtext 0.11.0 requires torch==1.10.0, but you have torch 1.11.0 which is incompatible.\u001B[0m\n",
      "Successfully installed torch-1.11.0 torchaudio-0.11.0 torchvision-0.12.0\n"
     ]
    }
   ],
   "source": [
    "!pip3 install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu111 --upgrade"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Tue Apr  5 05:42:55 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
      "| N/A   47C    P0    27W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "! nvidia-smi"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Requirement already satisfied: kaggle in /usr/local/lib/python3.7/dist-packages (1.5.12)\n",
      "Requirement already satisfied: tensorboard in /usr/local/lib/python3.7/dist-packages (2.8.0)\n",
      "Collecting wget\n",
      "  Downloading wget-3.2.zip (10 kB)\n",
      "Requirement already satisfied: urllib3 in /usr/local/lib/python3.7/dist-packages (from kaggle) (1.24.3)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from kaggle) (2021.10.8)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from kaggle) (2.23.0)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from kaggle) (4.63.0)\n",
      "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.7/dist-packages (from kaggle) (1.15.0)\n",
      "Requirement already satisfied: python-slugify in /usr/local/lib/python3.7/dist-packages (from kaggle) (6.1.1)\n",
      "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from kaggle) (2.8.2)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard) (0.4.6)\n",
      "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard) (3.17.3)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard) (1.35.0)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard) (57.4.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard) (0.6.1)\n",
      "Requirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard) (1.21.5)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard) (1.8.1)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard) (1.0.1)\n",
      "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard) (1.0.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard) (3.3.6)\n",
      "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard) (0.37.1)\n",
      "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard) (1.44.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard) (4.8)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard) (0.2.8)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard) (4.2.4)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard) (1.3.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard) (4.11.3)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard) (3.10.0.2)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard) (3.7.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard) (0.4.8)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->kaggle) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->kaggle) (2.10)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard) (3.2.0)\n",
      "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.7/dist-packages (from python-slugify->kaggle) (1.3)\n",
      "Building wheels for collected packages: wget\n",
      "  Building wheel for wget (setup.py) ... \u001B[?25l\u001B[?25hdone\n",
      "  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9675 sha256=06d2fc4046fc792b9336473a7052a9dd1d8c465f4f539f92b99603d9f6b9df80\n",
      "  Stored in directory: /root/.cache/pip/wheels/a1/b6/7c/0e63e34eb06634181c63adacca38b79ff8f35c37e3c13e3c02\n",
      "Successfully built wget\n",
      "Installing collected packages: wget\n",
      "Successfully installed wget-3.2\n",
      "Requirement already satisfied: tensorboard in /usr/local/lib/python3.7/dist-packages (2.8.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard) (3.3.6)\n",
      "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard) (0.37.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard) (0.4.6)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard) (1.8.1)\n",
      "Requirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard) (1.21.5)\n",
      "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard) (1.44.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard) (0.6.1)\n",
      "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard) (1.0.0)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard) (1.0.1)\n",
      "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard) (3.17.3)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard) (57.4.0)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard) (2.23.0)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard) (1.35.0)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from absl-py>=0.4->tensorboard) (1.15.0)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard) (4.2.4)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard) (4.8)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard) (0.2.8)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard) (1.3.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard) (4.11.3)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard) (3.10.0.2)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard) (3.7.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard) (0.4.8)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard) (2021.10.8)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard) (1.24.3)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard) (3.2.0)\n"
     ]
    }
   ],
   "source": [
    "! pip install kaggle tensorboard wget\n",
    "! pip install tensorboard --upgrade"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "! mkdir ~/.kaggle"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Downloading edible-wild-plants.zip to /content\n",
      " 99% 761M/766M [00:16<00:00, 29.2MB/s]\n",
      "100% 766M/766M [00:16<00:00, 47.9MB/s]\n"
     ]
    }
   ],
   "source": [
    "! cp kaggle.json ~/.kaggle/\n",
    "! chmod 600 ~/.kaggle/kaggle.json\n",
    "! kaggle datasets download -d gverzea/edible-wild-plants\n",
    "! unzip -q /content/edible-wild-plants.zip"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ],
      "application/javascript": [
       "\n",
       "        (async () => {\n",
       "            const url = new URL(await google.colab.kernel.proxyPort(6006, {'cache': true}));\n",
       "            url.searchParams.set('tensorboardColab', 'true');\n",
       "            const iframe = document.createElement('iframe');\n",
       "            iframe.src = url;\n",
       "            iframe.setAttribute('width', '100%');\n",
       "            iframe.setAttribute('height', '800');\n",
       "            iframe.setAttribute('frameborder', 0);\n",
       "            document.body.appendChild(iframe);\n",
       "        })();\n",
       "    "
      ]
     },
     "metadata": {}
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir logs"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Cloning into 'data-augmentation-benchmark'...\n",
      "remote: Enumerating objects: 185, done.\u001B[K\n",
      "remote: Counting objects: 100% (185/185), done.\u001B[K\n",
      "remote: Compressing objects: 100% (103/103), done.\u001B[K\n",
      "remote: Total 185 (delta 101), reused 156 (delta 72), pack-reused 0\u001B[K\n",
      "Receiving objects: 100% (185/185), 28.51 KiB | 3.56 MiB/s, done.\n",
      "Resolving deltas: 100% (101/101), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/alexfrst/data-augmentation-benchmark"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on cuda:0 \u001B[92m✓\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "sys.path.append(\"data-augmentation-benchmark\")\n",
    "\n",
    "import model.model as model\n",
    "from torch.optim import lr_scheduler\n",
    "from model.evaluate import evaluate\n",
    "from model.train import train_model\n",
    "from utils.dataset import load_dataset\n",
    "from utils.print_utils import Symbols\n",
    "from utils.tensorboard import tb_writer\n",
    "from augmentation.augmentation import convnext_best\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(f\"Working on {device} {Symbols.OK}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Input \u001B[1;32mIn [4]\u001B[0m, in \u001B[0;36m<cell line: 2>\u001B[1;34m()\u001B[0m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m transform, train_name \u001B[38;5;129;01min\u001B[39;00m transforms_list:\n\u001B[0;32m      3\u001B[0m     torch\u001B[38;5;241m.\u001B[39mcuda\u001B[38;5;241m.\u001B[39mempty_cache()\n\u001B[1;32m----> 5\u001B[0m     train_loader, val_loader, test_loader, nb_classes \u001B[38;5;241m=\u001B[39m \u001B[43mload_dataset\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mdatasets/dataset\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m24\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43madditional_transforms\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mtransform\u001B[49m\u001B[43m,\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      7\u001B[0m     convnext_small \u001B[38;5;241m=\u001B[39m model\u001B[38;5;241m.\u001B[39mload_convnext_small(nb_classes, transfer_learning\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[0;32m      9\u001B[0m     criterion \u001B[38;5;241m=\u001B[39m nn\u001B[38;5;241m.\u001B[39mCrossEntropyLoss()\n",
      "File \u001B[1;32m~\\Desktop\\deep_project\\utils\\dataset.py:96\u001B[0m, in \u001B[0;36mload_dataset\u001B[1;34m(train_image_directory, additional_transforms, batch_size)\u001B[0m\n\u001B[0;32m     94\u001B[0m nb_classes_train \u001B[38;5;241m=\u001B[39m check_labels_corectness(samples_train)\n\u001B[0;32m     95\u001B[0m nb_classes_val \u001B[38;5;241m=\u001B[39m check_labels_corectness(samples_val)\n\u001B[1;32m---> 96\u001B[0m nb_classes_test \u001B[38;5;241m=\u001B[39m \u001B[43mcheck_labels_corectness\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdataset_test\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     98\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNombre d\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mimages de train : \u001B[39m\u001B[38;5;132;01m%i\u001B[39;00m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m%\u001B[39m \u001B[38;5;28mlen\u001B[39m(dataset_train))\n\u001B[0;32m     99\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNombre d\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mimages de val : \u001B[39m\u001B[38;5;132;01m%i\u001B[39;00m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m%\u001B[39m \u001B[38;5;28mlen\u001B[39m(dataset_val))\n",
      "File \u001B[1;32m~\\Desktop\\deep_project\\utils\\dataset.py:38\u001B[0m, in \u001B[0;36mcheck_labels_corectness\u001B[1;34m(dataset)\u001B[0m\n\u001B[0;32m     37\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcheck_labels_corectness\u001B[39m(dataset):\n\u001B[1;32m---> 38\u001B[0m     labels \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39marray([x[\u001B[38;5;241m1\u001B[39m] \u001B[38;5;28;01mfor\u001B[39;00m x \u001B[38;5;129;01min\u001B[39;00m dataset])\n\u001B[0;32m     40\u001B[0m     nb_classes \u001B[38;5;241m=\u001B[39m labels\u001B[38;5;241m.\u001B[39mmax() \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m     42\u001B[0m     \u001B[38;5;28;01massert\u001B[39;00m labels\u001B[38;5;241m.\u001B[39mmin() \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m, \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mLabels must start at 0 \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mSymbols\u001B[38;5;241m.\u001B[39mFAIL\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n",
      "File \u001B[1;32m~\\Desktop\\deep_project\\utils\\dataset.py:38\u001B[0m, in \u001B[0;36m<listcomp>\u001B[1;34m(.0)\u001B[0m\n\u001B[0;32m     37\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcheck_labels_corectness\u001B[39m(dataset):\n\u001B[1;32m---> 38\u001B[0m     labels \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39marray([x[\u001B[38;5;241m1\u001B[39m] \u001B[38;5;28;01mfor\u001B[39;00m x \u001B[38;5;129;01min\u001B[39;00m dataset])\n\u001B[0;32m     40\u001B[0m     nb_classes \u001B[38;5;241m=\u001B[39m labels\u001B[38;5;241m.\u001B[39mmax() \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m     42\u001B[0m     \u001B[38;5;28;01massert\u001B[39;00m labels\u001B[38;5;241m.\u001B[39mmin() \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m, \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mLabels must start at 0 \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mSymbols\u001B[38;5;241m.\u001B[39mFAIL\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\AdIA\\lib\\site-packages\\torchvision\\datasets\\folder.py:230\u001B[0m, in \u001B[0;36mDatasetFolder.__getitem__\u001B[1;34m(self, index)\u001B[0m\n\u001B[0;32m    222\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    223\u001B[0m \u001B[38;5;124;03mArgs:\u001B[39;00m\n\u001B[0;32m    224\u001B[0m \u001B[38;5;124;03m    index (int): Index\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    227\u001B[0m \u001B[38;5;124;03m    tuple: (sample, target) where target is class_index of the target class.\u001B[39;00m\n\u001B[0;32m    228\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    229\u001B[0m path, target \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msamples[index]\n\u001B[1;32m--> 230\u001B[0m sample \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mloader\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpath\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    231\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtransform \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    232\u001B[0m     sample \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtransform(sample)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\AdIA\\lib\\site-packages\\torchvision\\datasets\\folder.py:269\u001B[0m, in \u001B[0;36mdefault_loader\u001B[1;34m(path)\u001B[0m\n\u001B[0;32m    267\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m accimage_loader(path)\n\u001B[0;32m    268\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 269\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mpil_loader\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpath\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\AdIA\\lib\\site-packages\\torchvision\\datasets\\folder.py:249\u001B[0m, in \u001B[0;36mpil_loader\u001B[1;34m(path)\u001B[0m\n\u001B[0;32m    247\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mopen\u001B[39m(path, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrb\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;28;01mas\u001B[39;00m f:\n\u001B[0;32m    248\u001B[0m     img \u001B[38;5;241m=\u001B[39m Image\u001B[38;5;241m.\u001B[39mopen(f)\n\u001B[1;32m--> 249\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mimg\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconvert\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mRGB\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\AdIA\\lib\\site-packages\\PIL\\Image.py:889\u001B[0m, in \u001B[0;36mImage.convert\u001B[1;34m(self, mode, matrix, dither, palette, colors)\u001B[0m\n\u001B[0;32m    847\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mconvert\u001B[39m(\u001B[38;5;28mself\u001B[39m, mode\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, matrix\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, dither\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, palette\u001B[38;5;241m=\u001B[39mWEB, colors\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m256\u001B[39m):\n\u001B[0;32m    848\u001B[0m     \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    849\u001B[0m \u001B[38;5;124;03m    Returns a converted copy of this image. For the \"P\" mode, this\u001B[39;00m\n\u001B[0;32m    850\u001B[0m \u001B[38;5;124;03m    method translates pixels through the palette.  If mode is\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    886\u001B[0m \u001B[38;5;124;03m    :returns: An :py:class:`~PIL.Image.Image` object.\u001B[39;00m\n\u001B[0;32m    887\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 889\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    891\u001B[0m     has_transparency \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39minfo\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtransparency\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m    892\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m mode \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmode \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mP\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[0;32m    893\u001B[0m         \u001B[38;5;66;03m# determine default mode\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\AdIA\\lib\\site-packages\\PIL\\ImageFile.py:253\u001B[0m, in \u001B[0;36mImageFile.load\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    247\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mOSError\u001B[39;00m(\n\u001B[0;32m    248\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mimage file is truncated \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    249\u001B[0m             \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m(\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mlen\u001B[39m(b)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m bytes not processed)\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    250\u001B[0m         )\n\u001B[0;32m    252\u001B[0m b \u001B[38;5;241m=\u001B[39m b \u001B[38;5;241m+\u001B[39m s\n\u001B[1;32m--> 253\u001B[0m n, err_code \u001B[38;5;241m=\u001B[39m \u001B[43mdecoder\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdecode\u001B[49m\u001B[43m(\u001B[49m\u001B[43mb\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    254\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m n \u001B[38;5;241m<\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[0;32m    255\u001B[0m     \u001B[38;5;28;01mbreak\u001B[39;00m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "results = {}\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "train_name = \"convnext_multi_augmentation\"\n",
    "\n",
    "train_loader, val_loader, test_loader, nb_classes = load_dataset(\"datasets/dataset\", batch_size=12, additional_transforms=convnext_best, augmentation_factor= 0.8)\n",
    "\n",
    "convnext_small = model.load_convnext_small(nb_classes, transfer_learning=False)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = optim.SGD(convnext_small.parameters(), lr=0.001, momentum=0.9)\n",
    "scheduler = lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.8)\n",
    "\n",
    "\n",
    "print(f\"Apprentissage en fine tunning avec convnext_small augment: {train_name}\")\n",
    "convnext_small.to(device)\n",
    "convnext_small.train(True)\n",
    "\n",
    "best_model, best_val_score =  train_model(convnext_small, train_loader, val_loader, optimizer, scheduler, criterion, evaluate, 65, device=device, tb_writer=tb_writer, training_name=\"convnext_small_\" + train_name)\n",
    "\n",
    "convnext_small.train(False)\n",
    "loss, accuracy = evaluate(best_model, test_loader, device, criterion)\n",
    "print(f\"Accuracy {train_name} (test): {(100 * accuracy):.1f}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "transforms_list = [\n",
    "    [transforms.RandomAutocontrast(p=0.8),\n",
    "     transforms.RandomAdjustSharpness(sharpness_factor=2.0),\n",
    "     transforms.RandomResizedCrop(size=(299, 299), scale=(0.45, 0.45))\n",
    "     ],\n",
    "    [transforms.RandomAutocontrast(p=0.8),\n",
    "     transforms.RandomAdjustSharpness(sharpness_factor=2.0),\n",
    "     transforms.RandomRotation(degrees=(0, 10))],\n",
    "    [transforms.ColorJitter(brightness=0.1, hue=.1),\n",
    "     transforms.GaussianBlur(kernel_size=(5, 9), sigma=(5, 10)),\n",
    "     transforms.RandomRotation(degrees=(0, 10))]\n",
    "]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "results = {}\n",
    "for index, transform in enumerate(transforms_list):\n",
    "    train_name = str(index)\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    train_loader, val_loader, test_loader, nb_classes = load_dataset(\"datasets/dataset\", batch_size=12, additional_transforms=transform, augmentation_factor= 0.8)\n",
    "\n",
    "    convnext_small = model.load_convnext_small(nb_classes, transfer_learning=False)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    optimizer = optim.SGD(convnext_small.parameters(), lr=0.001, momentum=0.9)\n",
    "    scheduler = lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.8)\n",
    "\n",
    "\n",
    "    print(f\"Apprentissage en fine tunning avec convnext_small augment: {train_name}\")\n",
    "    convnext_small.to(device)\n",
    "    convnext_small.train(True)\n",
    "\n",
    "    best_model, best_val_score =  train_model(convnext_small, train_loader, val_loader, optimizer, scheduler, criterion, evaluate, 65, device=device, tb_writer=tb_writer, training_name=\"convnext_small_\" + train_name)\n",
    "\n",
    "    convnext_small.train(False)\n",
    "    loss, accuracy = evaluate(best_model, test_loader, device, criterion)\n",
    "    results[train_name] = accuracy\n",
    "    print(f\"Accuracy {train_name} (test): {(100 * accuracy):.1f}\")\n",
    "print(results)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}