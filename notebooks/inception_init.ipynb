{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "! pip install kaggle wget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "! nvidia-smi"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "! mkdir ~ /.kaggle"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "! cp kaggle.json ~/.kaggle\n",
    "! chmod 600 ~/.kaggle/kaggle.json\n",
    "! kaggle datasets download -d gverzea/edible-wild-plants\n",
    "! unzip /content/edible-wild-plants.zip"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!git clone https://github.com/alexfrst/data-augmentation-benchmark"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on cuda:0 \u001B[92m✓\u001B[0m\n",
      "Loading dataset...\n",
      "Nombre d'images de train : 5902\n",
      "Nombre d'images de val : 656\n",
      "Nombre d'images de test : 310\n",
      "Apprentissage sur 62 classes\n",
      "Data loading and integrity check done \u001B[92m✓\u001B[0m\n",
      "Apprentissage en transfer learning\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0: 0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inception3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0: 1it [00:19, 19.04s/it, Acc (val)=0.0109, loss train=5.8, loss val=4.16]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inception3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0: 1it [00:30, 30.00s/it, Acc (val)=0.0109, loss train=5.8, loss val=4.16]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Input \u001B[1;32mIn [2]\u001B[0m, in \u001B[0;36m<cell line: 37>\u001B[1;34m()\u001B[0m\n\u001B[0;32m     34\u001B[0m inception_v3\u001B[38;5;241m.\u001B[39mtrain(\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[0;32m     35\u001B[0m torch\u001B[38;5;241m.\u001B[39mmanual_seed(\u001B[38;5;241m42\u001B[39m)\n\u001B[1;32m---> 37\u001B[0m \u001B[43mtrain_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43minception_v3\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mval_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcriterion\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mevaluate\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m65\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     38\u001B[0m inception_v3\u001B[38;5;241m.\u001B[39mtrain(\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[0;32m     39\u001B[0m loss, accuracy \u001B[38;5;241m=\u001B[39m evaluate(inception_v3, test_loader, device, criterion)\n",
      "File \u001B[1;32m~\\Desktop\\deep_project\\model\\train.py:22\u001B[0m, in \u001B[0;36mtrain_model\u001B[1;34m(model, loader_train, data_val, optimizer, criterion, evaluate, n_epochs, device, notebook)\u001B[0m\n\u001B[0;32m     20\u001B[0m     loss \u001B[38;5;241m=\u001B[39m criterion(outputs, labels)\n\u001B[0;32m     21\u001B[0m model\u001B[38;5;241m.\u001B[39mtrain(\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[1;32m---> 22\u001B[0m loss_val, accuracy \u001B[38;5;241m=\u001B[39m \u001B[43mevaluate\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdata_val\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcriterion\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     23\u001B[0m model\u001B[38;5;241m.\u001B[39mtrain(\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[0;32m     24\u001B[0m pbar\u001B[38;5;241m.\u001B[39mset_postfix(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m{\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mloss train\u001B[39m\u001B[38;5;124m\"\u001B[39m: loss\u001B[38;5;241m.\u001B[39mitem(), \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mloss val\u001B[39m\u001B[38;5;124m\"\u001B[39m: loss_val, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAcc (val)\u001B[39m\u001B[38;5;124m\"\u001B[39m: accuracy})\n",
      "File \u001B[1;32m~\\Desktop\\deep_project\\model\\evaluate.py:14\u001B[0m, in \u001B[0;36mevaluate\u001B[1;34m(model, dataset, device, criterion)\u001B[0m\n\u001B[0;32m     11\u001B[0m     _, preds \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mmax(outputs, \u001B[38;5;241m1\u001B[39m)\n\u001B[0;32m     12\u001B[0m     batch_acc \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39msum(preds \u001B[38;5;241m==\u001B[39m labels)\u001B[38;5;241m/\u001B[39m\u001B[38;5;28mlen\u001B[39m(preds)\n\u001B[1;32m---> 14\u001B[0m     avg_loss \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[43mloss\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mitem\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     15\u001B[0m     avg_accuracy \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m batch_acc\n\u001B[0;32m     17\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m avg_loss \u001B[38;5;241m/\u001B[39m \u001B[38;5;28mlen\u001B[39m(dataset), \u001B[38;5;28mfloat\u001B[39m(avg_accuracy) \u001B[38;5;241m/\u001B[39m \u001B[38;5;28mlen\u001B[39m(dataset)\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11: 70it [13:40, 11.72s/it, Acc (val)=0.447, loss train=2.6, loss val=2.36] \n",
      "Epoch 12: 70it [13:36, 11.66s/it, Acc (val)=0.462, loss train=3.3, loss val=2.32] \n",
      "Epoch 13: 70it [14:03, 12.05s/it, Acc (val)=0.453, loss train=2.89, loss val=2.29]\n",
      "Epoch 14: 6it [01:12, 11.70s/it, Acc (val)=0.46, loss train=3.18, loss val=2.28] "
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "sys.path.append(\"data-augmentation-benchmark\")\n",
    "\n",
    "import model.model as model\n",
    "from model.evaluate import evaluate\n",
    "from model.train import train_model\n",
    "from utils.dataset import load_dataset\n",
    "from utils.print_utils import Symbols\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(f\"Working on {device} {Symbols.OK}\")\n",
    "\n",
    "train_loader, val_loader, test_loader, nb_classes = load_dataset(\"datasets/dataset\", \"datasets/dataset-test\", batch_size=85)\n",
    "inception_v3 = model.load_inception_for_tl(nb_classes)\n",
    "scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode=\"min\", patience=5, factor=0.5, min_lr=1e-5)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "params_to_update = []\n",
    "for name,param in inception_v3.named_parameters():\n",
    "    if param.requires_grad == True:\n",
    "        params_to_update.append(param)\n",
    "\n",
    "optimizer = optim.SGD(params_to_update, lr=0.001, momentum=0.9)\n",
    "\n",
    "print(\"Apprentissage en transfer learning\")\n",
    "inception_v3.to(device)\n",
    "inception_v3.train(True)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "train_model(inception_v3, train_loader, val_loader, optimizer, criterion, evaluate, 65, device=device)\n",
    "inception_v3.train(False)\n",
    "loss, accuracy = evaluate(inception_v3, test_loader, device, criterion)\n",
    "print(\"Accuracy (test): %.1f%%\" % (100 * accuracy))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}